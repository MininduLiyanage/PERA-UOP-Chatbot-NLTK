{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dQgtqOxKnPab"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TazDVzDmnrdI"
   },
   "outputs": [],
   "source": [
    "f = open(\"/content/UOP.txt\",'r',errors = 'ignore')\n",
    "text_doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jDRJV5cpqZPQ",
    "outputId": "576d8a98-a0a6-464f-9260-26437fe53470"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_doc = text_doc.lower() #convert text to lowercase\n",
    "\n",
    "nltk.download('punkt')   #punkt tokenizer\n",
    "nltk.download('wordnet') #wordnet dictionary\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NzZKhAf2q-OQ"
   },
   "outputs": [],
   "source": [
    "sentence_tokens = nltk.sent_tokenize(text_doc) #tokenize into sentences\n",
    "word_tokens = nltk.word_tokenize(text_doc) #tokenize into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "2xAvMzbCrGEX",
    "outputId": "aec18eef-32ca-456d-a44f-5bfd2ce833e3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'the university of peradeniya hosts nine faculties, four postgraduate institutes (including the newly added postgraduate institute of medical sciences), 20 centres and units, 73 departments, and teaches about 12,000 students in the fields of medicine, agriculture, arts, science, engineering, dental sciences, veterinary medicine and animal science, management, and allied health science.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jPmWdpmrsl_A",
    "outputId": "d19aeb56-2d20-4405-c283-e6750c3d63b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crest',\n",
       " 'motto',\n",
       " 'sarvasva',\n",
       " 'locanam',\n",
       " 'sasthram',\n",
       " '(',\n",
       " 'sanskrit',\n",
       " ')',\n",
       " 'motto',\n",
       " 'in',\n",
       " 'english',\n",
       " 'knowledge']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[100:112]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSnPGFM8s2HD"
   },
   "source": [
    "# *Text Processing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PQek_z79spVh"
   },
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "B8CTePVWtX3F"
   },
   "outputs": [],
   "source": [
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fqSkmU5Ctcfe"
   },
   "outputs": [],
   "source": [
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9MiPpCluhQa"
   },
   "source": [
    "## Greeting coommands for ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kyX8m2eltdU3"
   },
   "outputs": [],
   "source": [
    "greet_user = (\"hello\", \"hi\",\"howdy\",\"hey\", \"greetings\", \"sup\", \"whassup\")\n",
    "\n",
    "greet_bot = [\"hi\", \"hey\", \"Welcome!\", \"hi there\", \"hello\", \"Good day!\",\"Hello there!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EH4iNlmcwzyp"
   },
   "outputs": [],
   "source": [
    "def greet(sentence):\n",
    "  for word in sentence.split():\n",
    "    if word.lower() in greet_user:\n",
    "      return random.choice(greet_bot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROC4Iqj1xoaB"
   },
   "source": [
    "#Intelligence of ChatBot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkOrOwm6ywHI"
   },
   "source": [
    "### 1. `TfidfVectorizer` in `sklearn`\n",
    "\n",
    "The `TfidfVectorizer` is a feature extraction method provided by the `scikit-learn` library, used to convert a collection of raw text documents into a matrix of TF-IDF (Term Frequency-Inverse Document Frequency) features. This transformation helps to quantify the importance of a word within a document relative to its occurrence across a collection of documents. Essentially, it scales down the impact of more frequently occurring words (which might be common across documents, like 'the' or 'and') and highlights more significant words that help to distinguish a document's content. This is particularly useful for applications like document classification, clustering, and information retrieval, where capturing the unique essence of each document in a large corpus is crucial.\n",
    "\n",
    "### 2. `cosine_similarity` in `sklearn`\n",
    "\n",
    " In the context of text processing, it is used to determine how similar two documents are, based on their vector representations (like those obtained from `TfidfVectorizer`). The cosine similarity value ranges from -1 to 1, where 1 indicates that the documents are identical in terms of their direction in the vector space (highly similar), 0 means they are orthogonal (completely different), and -1 indicates they are diametrically opposed. This metric is widely used in applications like document clustering, text mining, and recommendation systems, where understanding the degree of similarity between texts is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jA6y5Z7LxGro"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ipH0QWca7AIj"
   },
   "outputs": [],
   "source": [
    "def generate_response(user_response):\n",
    "  robo1_response=''\n",
    "  TfidVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english') #common English stop words (like 'the', 'is', etc.) are removed during tokenization.\n",
    "  tfidf = TfidVec.fit_transform(sentence_tokens) #computes TF-IDF matrix\n",
    "  vals = cosine_similarity(tfidf[-1], tfidf)  #computes cosine similarity between the TF-IDF vector of the userâ€™s input and all other sentence vectors\n",
    "  idx=vals.argsort()[0][-2]                   #sorts indices of the sentences based on cosine similarity scores. [0][-2] selects the second highest similarity,because the highest is the sentence itself\n",
    "  flat = vals.flatten()                       #converts the 2D array of cosine similarity scores into a 1D\n",
    "  flat.sort()                                 #sorts scores in ascending order\n",
    "  req_tfidf = flat[-2]                        #selects the second highest similarity score(check similarity)\n",
    "  if(req_tfidf==0):                           #0 means there is no meaningful similarity between user's input and any sentence\n",
    "    robo1_response=robo1_response+\"I am sorry! I don't understand you\"\n",
    "    return robo1_response\n",
    "  else:\n",
    "    robo1_response = robo1_response+sentence_tokens[idx]\n",
    "    return robo1_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrbiLsVJGgl8"
   },
   "source": [
    "##ChatBot Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VrP34OH7Jql",
    "outputId": "898dc5f7-e940-465a-f3b5-dbcd31ea2822"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT: Hello! I'm PERA, Official ChatBot of University of Peradeniya. I am here to provide required information about the university. If you want to end the coversation, type bye!\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"BOT: Hello! I'm PERA, Official ChatBot of University of Peradeniya. I am here to provide required information about the university. If you want to end the coversation, type bye!\")\n",
    "while(flag==True):\n",
    "  user_response = input()\n",
    "  user_response=user_response.lower()\n",
    "  if(user_response!='bye!'):\n",
    "    if(user_response=='thanks' or user_response=='thank you' ):\n",
    "      print(\"PERA: No worries. How else I can assist you?\")\n",
    "    else:\n",
    "      if(greet(user_response)!=None):\n",
    "        print(\"PERA: \"+greet(user_response))\n",
    "      else:\n",
    "        sentence_tokens.append(user_response)\n",
    "        word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
    "        final_words=list(set(word_tokens))\n",
    "        print(\"PERA: \", end=\"\")\n",
    "        print(generate_response(user_response))\n",
    "        sentence_tokens.remove(user_response)\n",
    "  else:\n",
    "    flag=False\n",
    "    print(\"PERA: Goodbye! Have a Nice day \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpFhENFBNE_1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qs28w5wEH0Ek"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
